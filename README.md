# evaluation
Submission for Evaluation part of course 8730 - Natural Language Processing and Understanding

Folder Structure
-> data
    -> reference
        -> reference.txt (Contains the correct answers for the test set)
    -> results
        -> t5_small_500.txt (Contains the answers for the test set generated by t5_small_500 model)
        -> t5_small_1000.txt (Contains the answers for the test set generated by t5_small_1000 model)
        -> t5_small_1500.txt (Contains the answers for the test set generated by t5_small_1500 model)
        -> facebook_bart_200.txt (Contains the answers for the test set generated by facebook_bart_200 model)
    -> plots (Contains the plots for the evaluation)

Metrices used for evaluation
-> BLEU
-> METEOR
-> Edit Distance

How to run the code using python environment
-> Install the requirements using the command `pip install -r requirements.txt`
`pip install -r requirements.txt`
-> Run the evaluation script using the command
`python evaluate.py`

Note: The conda environment was used during the development of the code. The requirement file contains the packages used in the conda environment. The code can be run using the python environment as well.

The results for the evaluation are produced using huggingface libraries. The codes for the same were references from the following links:
-> https://huggingface.co/docs/transformers/model_doc/t5
-> https://huggingface.co/docs/transformers/model_doc/bart

The codes were executed in Google colab and the python files for the same are given in the ref_codes folder.

main.py will generate the plots of edit distance, bleu and meteor scores for the models. It takes the reference file and results from the data folder.
time_plot.py and loss_plot.py take static data as an input and generate the plots for the same. They need to be run separately.