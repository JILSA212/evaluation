# -*- coding: utf-8 -*-
"""Statement_50_processed_500.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xuCvzZCBipJaa_ymAKRcSHDQnuQafO19
"""

! pip install huggingface_hub

! huggingface-cli login

from google.colab import drive
drive.mount('/content/drive')

! pip install transformers

!pip install sentencepiece

from transformers import T5Tokenizer, T5ForConditionalGeneration
# from transformers import T5Tokenizer, T5Model
import torch

tokenizer = T5Tokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")

# the following 2 hyperparameters are task-specific
max_source_length = 512
max_target_length = 128

# Suppose we have the following 2 training examples:
# input_sequence_1 = "Welcome to NYC"
# output_sequence_1 = "Bienvenue Ã  NYC"

# input_sequence_2 = "HuggingFace is a company"
# output_sequence_2 = "HuggingFace est une entreprise"

# encode the inputs
# task_prefix = "translate English to French: "
# input_sequences = [input_sequence_1, input_sequence_2]

import pandas as pd

df_train = pd.read_csv("drive/MyDrive/huggingface_data/statement_50_processed/statement_50_processed_train.csv")
df_eval = pd.read_csv("drive/MyDrive/huggingface_data/statement_50_processed/statement_50_processed_eval.csv")

print(df_train.shape)

input_sequences = df_train.x.values.tolist()
output_sequences = df_train.y.values.tolist()
# print(df_train.x.values.tolist())

eval_input_sequences = df_eval.x.values.tolist()
eval_output_sequences = df_eval.y.values.tolist()

encoding = tokenizer(
    # [task_prefix + sequence for sequence in input_sequences],
    [sequence for sequence in input_sequences],
    padding="longest",
    max_length=max_source_length,
    truncation=True,
    return_tensors="pt",
)

eval_encoding = tokenizer(
    # [task_prefix + sequence for sequence in input_sequences],
    [sequence for sequence in eval_input_sequences],
    padding="longest",
    max_length=max_source_length,
    truncation=True,
    return_tensors="pt",
)

input_ids, attention_mask = encoding.input_ids, encoding.attention_mask
eval_input_ids, eval_attention_mask = eval_encoding.input_ids, eval_encoding.attention_mask

# encode the targets
target_encoding = tokenizer(
    output_sequences,
    padding="longest",
    max_length=max_target_length,
    truncation=True,
    return_tensors="pt",
)
labels = target_encoding.input_ids

eval_target_encoding = tokenizer(
    eval_output_sequences,
    padding="longest",
    max_length=max_target_length,
    truncation=True,
    return_tensors="pt",
)
eval_labels = target_encoding.input_ids

# replace padding token id's of the labels by -100 so it's ignored by the loss
labels[labels == tokenizer.pad_token_id] = -100
eval_labels[eval_labels == tokenizer.pad_token_id] = -100

new_input_train = []

for i in range(len(input_ids)):
  temp_dict = {}
  temp_dict["input_ids"] = input_ids[i]
  temp_dict["attention_mask"] = attention_mask[i]
  temp_dict["labels"] = labels[i]
  new_input_train.append(temp_dict)

new_input_eval = []

for i in range(len(eval_input_ids)):
  temp_dict = {}
  temp_dict["input_ids"] = eval_input_ids[i]
  temp_dict["attention_mask"] = eval_attention_mask[i]
  temp_dict["labels"] = eval_labels[i]
  new_input_eval.append(temp_dict)

# forward pass
# loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss
# loss.item()

# input_ids = tokenizer("translate English to German: The house is wonderful.", return_tensors="pt").input_ids
# labels = tokenizer("Das Haus ist wunderbar.", return_tensors="pt").input_ids

!pip install evaluate

from transformers import TrainingArguments, Trainer
import numpy as np
import evaluate

metric = evaluate.load("accuracy")

training_args = TrainingArguments(output_dir="./statement_50_processed/", learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=500, push_to_hub=True)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

trainer = Trainer(
    model=model,
    args=training_args,
    # train_dataset=small_train_dataset,
    train_dataset=new_input_train,
    eval_dataset=new_input_eval,
    compute_metrics=compute_metrics,
)

trainer.train()

# loss = model(input_ids=input_ids, labels=labels).loss
# loss.item()

trainer.save_model(".")

import pandas as pd

df_test = pd.read_csv("drive/MyDrive/huggingface_data/statement_50_processed/statement_50_processed_test.csv")

test_input_sequences = df_test.x.values.tolist()
test_output_sequences = df_test.y.values.tolist()

# inputs = tokenizer([task_prefix + sentence for sentence in test_input_sentences], return_tensors="pt", padding=True)
inputs = tokenizer([sentence for sentence in test_input_sequences], return_tensors="pt", padding=True)

output_sequences = model.generate(
    input_ids=inputs["input_ids"],
    attention_mask=inputs["attention_mask"],
    do_sample=False,  # disable sampling to test if batching affects output
)

result = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)

result

filename = "results.txt"

f=open(filename, "w")
for line in result:
  f.write(str(line) + "\n")
f.close()

trainer.push_to_hub()

from huggingface_hub import notebook_login

notebook_login()